# -*- coding: utf-8 -*-
from pyspark.sql.functions import col, current_timestamp, regexp_extract, window
from utils import SPARK_DATA_PATH, get_spark_session

# Create a SparkSession (the config bit is only for Windows!)
spark = get_spark_session('Windowed Structured Streaming')

# Monitor the logs directory for new log data, and read in the raw lines as accessLines
accessLines = spark.readStream.text(f"{SPARK_DATA_PATH}/logs")

# Parse out the common log format to a DataFrame
contentSizeExp = r'\s(\d+)$'
statusExp = r'\s(\d{3})\s'
generalExp = r'\"(\S+)\s(\S+)\s*(\S*)\"'
timeExp = r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2} -\d{4})]'
hostExp = r'(^\S+\.[\S+\.]+\S+)\s'

logsDF = accessLines.select(
    regexp_extract('value', hostExp, 1).alias('host'),
    regexp_extract('value', timeExp, 1).alias('timestamp'),
    regexp_extract('value', generalExp, 1).alias('method'),
    regexp_extract('value', generalExp, 2).alias('endpoint'),
    regexp_extract('value', generalExp, 3).alias('protocol'),
    regexp_extract('value', statusExp, 1).cast('integer').alias('status'),
    regexp_extract('value', contentSizeExp, 1).cast('integer').alias('content_size')
)
logsDF = logsDF.withColumn('eventTime', current_timestamp())

# Keep a running count of every access by status code
endpointCounts = logsDF.groupBy(
    window(
        col('eventTime'),
        windowDuration='30 seconds',
        slideDuration='10 seconds',
    ),
    col('endpoint')
).count()
endpointCounts.orderBy(col('count').desc())

# Kick off our streaming query, dumping results to the console
query = endpointCounts.writeStream.outputMode("complete").format("console").queryName("counts").start()

# Run forever until terminated
query.awaitTermination()

# Cleanly shut down the session
spark.stop()

